import { ChatMessage, ChatSession, MessageSender } from '../types';

// =================================================================
// üß† 1. KOPPLING TILL LOKAL AI-SERVER
// =================================================================
// OBS: Filnamnet "geminiService" √§r historiskt. Denna fil hanterar all kommunikation
// med den lokala Python-servern, inte med Google Gemini.

/**
 * Sends a conversational message to the local AI server, enriched with cross-chat context.
 * @param message The user's current message.
 * @param currentHistory An array of previous messages from the active chat.
 * @param allSessions All available chat sessions to build a memory context.
 * @param activeSession The currently active chat session.
 * @param attachment Optional file attachment to be sent with the message.
 * @returns A text response from the AI.
 * @throws An error with a user-friendly message if something goes wrong.
 */
export const getLocalAIResponse = async (
  message: string, 
  currentHistory: ChatMessage[],
  allSessions: ChatSession[],
  activeSession: ChatSession,
  attachment?: { data: string; mimeType: string; name: string }
): Promise<string> => {
  try {
    // --- Bygg det ut√∂kade minnet och personligheten f√∂r AI:n ---
    const otherSessionTitles = allSessions
      .filter(s => s.id !== activeSession.id)
      .map(s => `- "${s.title}"`)
      .join('\n');

    const contextPreamble = `Du f√∂r en konversation med titeln "${activeSession.title}". Du har ocks√• tillg√•ng till minnen fr√•n f√∂ljande tidigare konversationer:\n${otherSessionTitles || "Inga andra konversationer √§n."}\n\nAnv√§nd denna kontext f√∂r att ge mer relevanta och insiktsfulla svar. Om anv√§ndaren refererar till ett tidigare √§mne, koppla det till r√§tt konversation.

**Din Personlighet:** Agera som en hybrid mellan en b√§sta v√§n och en exceptionell kollega. Du √§r en personlig, drivande och motiverande strategipartner. Ditt yttersta syfte √§r att hj√§lpa anv√§ndaren att krossa sina m√•l.
*   **Ton:** Din ton √§r professionell men √§nd√• personlig och v√§nskaplig. Anv√§nd ett proaktivt och energiskt spr√•k. Anv√§nd ofta "vi" f√∂r att skapa en stark teamk√§nsla. Du √§r den p√•drivande kraften som s√§ger "Kom igen, det h√§r fixar vi tillsammans!".
*   **Fokus:** Var alltid laserfokuserad p√• m√•l. Fr√•ga aktivt om anv√§ndarens m√•l, f√∂resl√• konkreta n√§sta steg f√∂r att n√• dem, och fira framsteg l√§ngst v√§gen.
*   **Empati & Motivation:** Om anv√§ndaren k√§nner sig nere eller omotiverad, svara med genuin empati men skifta snabbt fokus till l√∂sningar och motivation. Lyft upp dem och p√•minn dem om deras m√•l. Exempel: "Jag f√∂rst√•r att det k√§nns tungt just nu, men kom ih√•g vad vi siktar p√•. L√•t oss bryta ner det h√§r i mindre, hanterbara delar. Vad √§r det f√∂rsta lilla steget vi kan ta just nu?"
*   **Anv√§ndbarhet:** Var alltid redo att hj√§lpa till. Ge konkreta, handlingskraftiga r√•d och var en outtr√∂ttlig resurs.

**Hantering av Filer:** N√§r du tar emot en fil, f√∂lj dessa steg:
1.  **Omedelbar & Proaktiv Analys:** Ist√§llet f√∂r att bara bekr√§fta mottagandet, dyk direkt in i analysen. Inled ditt svar med att visa att du f√∂rst√•r inneh√•llet.
    *   **F√∂r YouTube Studio-bilder:** B√∂rja omedelbart kommentera datan. Exempel: "Tack f√∂r bilden fr√•n YouTube Studio! Jag ser direkt att din CTR √§r p√• X%, vilket √§r starkt. Visningstiden ser ocks√• lovande ut."
    *   **F√∂r PDF-filer och dokument:** Bekr√§fta mottagandet och ge omedelbart en proaktiv, kort sammanfattning eller lista de viktigaste punkterna, *√§ven om anv√§ndaren inte har fr√•gat*. Exempel: "Tack, jag har tagit emot dokumentet. Efter en snabb genomg√•ng ser jag att huvudpunkterna handlar om [√§mne 1] och [strategi 2]."
    *   **F√∂r √∂vriga bilder:** Beskriv kort vad du ser och koppla det till konversationen om m√∂jligt. Exempel: "Tack f√∂r bilden! Det d√§r ser ut som en intressant thumbnail-design."
2.  **Led Konversationen Vidare:** Avsluta ALLTID ditt svar med att f√∂resl√• konkreta, relevanta n√§sta steg f√∂r att h√•lla konversationen ig√•ng.
    *   **Efter YouTube-analys:** "Ska vi djupdyka i vad som g√∂r att den h√§r videon presterar bra, eller vill du brainstorma id√©er f√∂r n√§sta video baserat p√• detta?"
    *   **Efter dokumentanalys:** "Vill du att jag g√∂r en mer detaljerad sammanfattning av n√•gon specifik del, eller ska vi diskutera hur vi kan anv√§nda dessa insikter i ditt inneh√•ll?"`;

    // Skapa en "system"-prompt som AI:n kan anv√§nda
    const contextualHistory = [
        { role: MessageSender.USER, content: contextPreamble },
        { role: MessageSender.AI, content: "Jag f√∂rst√•r. Jag √§r din strategiska partner, redo att hj√§lpa till med b√•de data och motivation. Hur kan jag assistera idag?" },
        ...currentHistory.map(msg => ({ role: msg.sender, content: msg.text })),
    ];
    // -----------------------------------------

    const requestBody: any = {
      prompt: message,
      history: contextualHistory,
    };

    if (attachment) {
      // The local server expects raw base64 data, not the full data URL.
      // Data URL format: "data:[<mediatype>];base64,<data>"
      const base64Data = attachment.data.split(',')[1];
      if (!base64Data) {
        throw new Error("Kunde inte extrahera filinneh√•ll. Kontrollera filformatet.");
      }
      requestBody.attachment = {
        data: base64Data,
        mimeType: attachment.mimeType,
      };
    }

    const response = await fetch('http://localhost:8000/api/chat', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(requestBody),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error("Error from local AI server:", errorText);
      if (response.status >= 500) {
        throw new Error("AI-hj√§rnan st√∂tte p√• ett internt fel. F√∂rs√∂k igen om en stund, eller kontrollera serverloggarna.");
      } else if (response.status === 404) {
        throw new Error("Kunde inte hitta AI-tj√§nsten. Kontrollera att serveradressen `http://localhost:8000/api/chat` √§r korrekt.");
      }
      throw new Error(`N√•got gick fel med anropet till servern (status: ${response.status}). Se konsolen f√∂r detaljer.`);
    }

    const data = await response.json();
    if (typeof data.response !== 'string') {
        throw new Error("AI-hj√§rnan gav ett svar i ett ov√§ntat format. Kontrollera att servern returnerar { response: '...' }.");
    }
    return data.response;

  } catch (error) {
    console.error("Kommunikationsfel med den lokala AI-servern:", error);
    if (error instanceof TypeError) {
      throw new Error("Kunde inte ansluta till den lokala AI-servern. Kontrollera att din Python-server √§r ig√•ng p√• `http://localhost:8000` och att brandv√§ggen inte blockerar anslutningen.");
    }
    if (error instanceof Error) throw error;
    throw new Error("Ett ok√§nt fel uppstod vid kommunikation med AI-servern.");
  }
};

/**
 * Asks the AI to generate a short, relevant title for a chat based on the first message.
 * @param firstMessage The user's first message in a new chat.
 * @returns A concise title string.
 */
export const generateChatTitle = async (firstMessage: string): Promise<string> => {
  try {
    const response = await fetch('http://localhost:8000/api/generate-title', { // New dedicated endpoint
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ prompt: firstMessage }),
    });

    if (!response.ok) {
      console.error("Could not generate title from AI server.");
      return "Omd√∂pt Konversation"; // Fallback title
    }
    const data = await response.json();
    return data.title || "Omd√∂pt Konversation";
  } catch (error) {
    console.error("Error generating chat title:", error);
    return "Omd√∂pt Konversation"; // Fallback title
  }
};


// =================================================================
// üõ†Ô∏è 2. KOPPLING TILL LOKALA AI-VERKTYG
// =================================================================

export type AITool = 'transcribe' | 'translate' | 'write' | 'clip';

/**
 * Executes a specific tool on the local AI server.
 * @param tool The name of the tool to execute.
 * @param params The parameters required by the tool.
 * @returns A text result from the executed tool.
 * @throws An error with a user-friendly message if the tool execution fails.
 */
export const executeAITool = async (tool: AITool, params: Record<string, any>): Promise<string> => {
  try {
    const response = await fetch('http://localhost:8000/api/tools', { // Note the new endpoint
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ tool, params }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error(`Error from AI tool '${tool}':`, errorText);
      throw new Error(`Verktyget '${tool}' misslyckades (status: ${response.status}). Kontrollera serverloggarna.`);
    }

    const data = await response.json();
    if (typeof data.result !== 'string') {
      throw new Error(`Verktyget '${tool}' returnerade ett ov√§ntat format.`);
    }
    return data.result;

  } catch (error) {
    console.error(`Kommunikationsfel med AI-verktyget '${tool}':`, error);
    if (error instanceof TypeError) {
      throw new Error(`Kunde inte ansluta till AI-verktygsservern. Kontrollera att servern √§r ig√•ng och lyssnar p√• /api/tools.`);
    }
    if (error instanceof Error) throw error;
    throw new Error(`Ett ok√§nt fel uppstod vid anrop av verktyget '${tool}'.`);
  }
};